{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "flaml.automl is not available. Please install flaml[automl] to enable AutoML functionalities.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import autogen\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_community.document_loaders.image import UnstructuredImageLoader\n",
    "from autogen import register_function\n",
    "from typing import Annotated, Literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse_files_in_directory(directory_path):\n",
    "    \n",
    "#     parsed_data = parse_pdfs(directory_path)\n",
    "\n",
    "#     # for filename in os.listdir(directory_path):\n",
    "#         # file_path = os.path.join(directory_path, filename)\n",
    "        \n",
    "#         # if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp')):\n",
    "#         #     loader = UnstructuredImageLoader(file_path)\n",
    "#         #     documents = loader.load()\n",
    "#         #     parsed_text = ' '.join([doc.page_content for doc in documents])\n",
    "\n",
    "#         # else:\n",
    "#         #     continue\n",
    "\n",
    "#         # parsed_data[filename] = parsed_text\n",
    "\n",
    "#     return parsed_data\n",
    "\n",
    "\n",
    "def parse_pdfs(directory:str =\"../data/\") -> dict:\n",
    "# def parse_pdfs(dir_path=\"../data/\"):\n",
    "    parsed_data = {}\n",
    "    loader = PyPDFDirectoryLoader(directory, extract_images=True)\n",
    "    documents = loader.load()\n",
    "    for page in documents:\n",
    "        if parsed_data.get(page.metadata[\"source\"]) is None:\n",
    "            parsed_data[page.metadata[\"source\"]] = [page.page_content]\n",
    "        else:\n",
    "            parsed_data[page.metadata[\"source\"]].append(page.page_content)\n",
    "    return parsed_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.environ.get(\"GROQ_API_KEY\")\n",
    "\n",
    "config_list = [\n",
    "  {\n",
    "    \"model\": \"llama-3.2-3b-preview\",\n",
    "    \"base_url\": \"https://api.groq.com/openai/v1/chat/completions\",\n",
    "    \"api_type\": \"groq\",\n",
    "    \"api_key\": api_key,\n",
    "    \"api_rate_limit\": 100\n",
    "  }\n",
    "]\n",
    "\n",
    "# llm_config = config_list[0]\n",
    "llm_config={\"config_list\": config_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"Parse the documents and classify each of them by a heading\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_proxy = autogen.ConversableAgent(\n",
    "#     name=\"Admin\",\n",
    "#     system_message=\"Give the task, and send instructions to doc_classifier to classify the documents in the data folder\",\n",
    "#     code_execution_config=False,\n",
    "#     llm_config=llm_config,\n",
    "#     human_input_mode=\"ALWAYS\",\n",
    "# )\n",
    "\n",
    "user_proxy = autogen.ConversableAgent(\n",
    "    name=\"User\",\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\") and x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "    human_input_mode=\"NEVER\",\n",
    "    # max_consecutive_auto_reply=10,\n",
    "    # code_execution_config={\n",
    "    #     \"work_dir\": \"code\",\n",
    "    #     \"use_docker\": False\n",
    "    # },\n",
    "    llm_config=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_parse_caller = autogen.ConversableAgent(\n",
    "    name=\"Doc Parse Caller\",\n",
    "    llm_config=llm_config,\n",
    "    description=\"A caller for parsing documents\",\n",
    "    system_message=\"You are a helpful AI assistant. Suggest how to call the python function parse_pdfs(directory)\"\n",
    ")\n",
    "\n",
    "doc_parser = autogen.ConversableAgent(\n",
    "    name=\"Doc Parser\",\n",
    "    llm_config=llm_config,\n",
    "    description=\"A parser that executes the parser function\",\n",
    "    # system_message=\"You are a doc parser. You only execute the python function call suggested to you by the Doc Parse Caller.\"\n",
    "    # system_message=\"You are a helpful AI assistant as an executer that parses documents. If you receive a python call instruction, then you execute it, or else ignore the message.\"\n",
    ")\n",
    "\n",
    "doc_classifier = autogen.ConversableAgent(\n",
    "    name=\"Doc Classifier\",\n",
    "    llm_config=llm_config,\n",
    "    description=\"A classifier that classifies the text that has been parsed\",\n",
    "    system_message=\"You are a helpful AI assistant.\"\n",
    "                   \"For each document provided, categorize them from one of these options ['Passport', 'Residence Permit', 'Transaction', 'Health Insurance', 'Other'].\"\n",
    "                   \"Reply TERMINATE when the task is done.\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_proxy.register_function(\n",
    "#     function_map={\n",
    "#         \"parse_pdfs\": parse_pdfs()\n",
    "#     }\n",
    "# )\n",
    "\n",
    "register_function(\n",
    "    parse_pdfs,\n",
    "    caller=doc_parse_caller,  \n",
    "    executor=doc_parser,  \n",
    "    name=\"parse_pdfs\",  \n",
    "    description=\"A simple document parser\",  # A description of the tool.\n",
    ")\n",
    "\n",
    "# # Register the tool signature with the assistant agent.\n",
    "# doc_classifier.register_for_llm(name=\"parse_pdfs\", description=\"A simple pdf parser\")(parse_pdfs)\n",
    "\n",
    "# # Register the tool function with the user proxy agent.\n",
    "# user_proxy.register_for_execution(name=\"parse_pdfs\")(parse_pdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupchat = autogen.GroupChat(\n",
    "    agents=[user_proxy, doc_parse_caller, doc_parser, doc_classifier],\n",
    "    messages=[],\n",
    "    max_round=5,\n",
    "    allowed_or_disallowed_speaker_transitions={\n",
    "        user_proxy: [doc_parser],\n",
    "        doc_parser: [doc_parse_caller, doc_classifier],\n",
    "        doc_parse_caller: [doc_parser],\n",
    "        doc_classifier: [user_proxy],\n",
    "    },\n",
    "    speaker_transitions_type=\"allowed\",\n",
    ")\n",
    "\n",
    "manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'function',\n",
       "  'function': {'description': 'A simple document parser',\n",
       "   'name': 'parse_pdfs',\n",
       "   'parameters': {'type': 'object',\n",
       "    'properties': {'directory': {'type': 'string',\n",
       "      'default': '../data/',\n",
       "      'description': 'directory'}},\n",
       "    'required': []}}}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_parse_caller.llm_config[\"tools\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_calling(recipient, messages, sender, config):\n",
    "    print(\"Reflecting...\", \"yellow\")\n",
    "    # return f\"Can you provide the python function to call for parsing the documents?\\n\\n {recipient.chat_messages_for_summary(sender)[-1]['content']}\"\n",
    "    return f\"{recipient.chat_messages_for_summary(sender)[-1]['content']}\"\n",
    "\n",
    "\n",
    "manager.register_nested_chats(\n",
    "    [\n",
    "        {\n",
    "            \"sender\": doc_parser,\n",
    "            \"recipient\": doc_parse_caller,\n",
    "            \"message\": parse_calling,\n",
    "            \"max_turns\": 2,\n",
    "            \"summary_method\": \"last_msg\",\n",
    "        }\n",
    "    ],\n",
    "    trigger=user_proxy,  # condition=my_condition,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser\u001b[0m (to chat_manager):\n",
      "\n",
      "I have some pdf documents in the directory '../data/', and I would like to know what type of documents they are (in terms of classification)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Doc Parser\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mDoc Parser\u001b[0m (to chat_manager):\n",
      "\n",
      "To determine the type of PDF documents in your directory, you can use the `pyPDF2` and `openpyxl` libraries in Python. These libraries can extract information about the PDF documents, such as their authors, subject, and pages. However, for classifier-based approach, you can use:\n",
      "\n",
      "*   `python-magic` library, which can identify the type of PDF documents based on their metadata.\n",
      "\n",
      "Here's a step-by-step example using `python-magic` library:\n",
      "\n",
      "### Install the required library\n",
      "\n",
      "```bash\n",
      "pip install python-magic\n",
      "```\n",
      "\n",
      "### Python script to classify PDF documents\n",
      "\n",
      "```python\n",
      "import magic\n",
      "\n",
      "def classify_pdfs(directory):\n",
      "    # Create a Magic object\n",
      "    m = magicMagic()\n",
      "\n",
      "    # Iterate over all files in the directory\n",
      "    for filename in [f for f in directory.find() if f.is_file()]:\n",
      "        filepath = filename.path\n",
      "        # Get the magic type of the file\n",
      "        mime_type = m.from_file(filepath)\n",
      "\n",
      "        # Determine the document type from the MIME type\n",
      "        if \"application/pdf\" in mime_type:\n",
      "            print(f\"{filename.name}: PDF Document (Potential {mime_type})\")\n",
      "        else:\n",
      "            print(f\"{filename.name}: {mime_type}\")\n",
      "\n",
      "# Set the directory containing the PDF documents\n",
      "directory = '../data/'\n",
      "\n",
      "# Classify the PDF documents in the directory\n",
      "classify_pdfs(directory)\n",
      "```\n",
      "\n",
      "However, `python-magic` does not work for PDF documents because, as you can see in the above discussion, whether the results are useful or not when considering these files is always questionable due to the often varied nature of these document files.\n",
      "\n",
      "The solution I will propose is `pdfminer` and `pdf2image` libraries together which can be used to classify the documents by analyzing the documents.\n",
      "\n",
      "First, install the required libraries:\n",
      "\n",
      "```bash\n",
      "pip install pdfminer pdf2image\n",
      "```\n",
      "\n",
      "Here is an example of how to use these libraries to classify PDF documents:\n",
      "\n",
      "### Python script to classify PDF documents\n",
      "\n",
      "```python\n",
      "from pdfminer.converter import TextConverter\n",
      "from pdfminer.layout import LAParams\n",
      "from pdfminer.pdfdocument import PDFDocument\n",
      "from pdfminer.pdfinterceptor import PDFPageAggregator\n",
      "from pdfminer.pdfparser import PDFParser\n",
      "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
      "from pdf2image import convert_from_path\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "\n",
      "# Set the directory containing the PDF documents\n",
      "directory = '../data/'\n",
      "\n",
      "# Define classes for the documents\n",
      "document_classes = ['article', 'book', 'academic', 'etc']\n",
      "\n",
      "# Initialize the list to hold the classified documents\n",
      "classified_documents = []\n",
      "\n",
      "# Convert the PDF documents to text and classify them\n",
      "for filename in [f for f in directory.find() if f.is_file()]:\n",
      "    filepath = filename.path\n",
      "\n",
      "    # Extract the text of the PDF document\n",
      "    pages = convert_from_path(filepath)\n",
      "    text = ''\n",
      "    for page in pages:\n",
      "        text += page.extract_text()\n",
      "\n",
      "    # Use TF-IDF vectorization to process the text\n",
      "    vectorizer = TfidfVectorizer()\n",
      "    tfidf = vectorizer.fit_transform([text])\n",
      "    vector = tfidf.toarray()[0]\n",
      "\n",
      "    # Use Naive Bayes classifier to predict the document type\n",
      "    clf = MultinomialNB().fit(vectorizer.components_)\n",
      "    predicted = clf.predict(vector)\n",
      "    predicted_class = predicted[0]\n",
      "\n",
      "    # Append the classified document to the list\n",
      "    classified_documents.append({\n",
      "        'filename': filename.name,\n",
      "        'document_class': predicted_class,\n",
      "    })\n",
      "\n",
      "# Print the classified documents\n",
      "for document in classified_documents:\n",
      "    print(f\"Filename: {document['filename']}, Document Class: {document['document_class']}\")\n",
      "```\n",
      "\n",
      "**Please note that for the provided example script to function smoothly the directory '../data/' should contain only PDF documents.**\n",
      "\n",
      "The script works by first converting each PDF document into text, then it processes the text into a TF-IDF vector and uses it for classification. This classification system is based on text classification, but it can be adapted to different classification systems according to your requirements.\n",
      "\n",
      "**Please note that you need to run this script at your own risk.**\n",
      "\n",
      "The quality and relevance of the classification results may vary depending on the text and the complexity of the PDF documents being classified.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Doc Parse Caller\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mDoc Parse Caller\u001b[0m (to chat_manager):\n",
      "\n",
      "<function=pdfminer.parse_pdfs>` {\"path\": \"../data/\"}`\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Doc Parser\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mDoc Parser\u001b[0m (to chat_manager):\n",
      "\n",
      "It seems that you're calling the `pdfminer.parse_pdfs` function with an abstract path.\n",
      "\n",
      "The `pdfminer.parse_pdfs` function requires a string or a file-like object that represents the path to the directory containing the PDF files.\n",
      "\n",
      "Instead of passing the directory object directly, you can pass the path to the directory as a string by calling the `find()` method on the directory object.\n",
      "\n",
      "Here's how you can modify your code to pass the path as a string:\n",
      "\n",
      "```python\n",
      "import magic\n",
      "\n",
      "directory = '../data/'\n",
      "files = [f for f in directory.find() if f.is_file()]\n",
      "for f in files:\n",
      "    filepath = f.path\n",
      "    mime_type = magic.from_file(filepath)\n",
      "    if \"application/pdf\" in mime_type:\n",
      "        print(f\"{f.name}: PDF Document (Potential {mime_type})\")\n",
      "    else:\n",
      "        print(f\"{f.name}: {mime_type}\")\n",
      "```\n",
      "\n",
      "This code first finds the files in the specified directory using the `find()` method, and then for each file, it calls the `from_file()` method on the `magic` object to get the MIME type of the file.\n",
      "\n",
      "However, this method does not allow you to process the files recursively.\n",
      "\n",
      "Here is an example using the following approach:\n",
      "\n",
      "```python\n",
      "import os\n",
      "import magic\n",
      "import glob\n",
      "\n",
      "directory = '../data/'\n",
      "\n",
      "def process_pdfs(directory):\n",
      "    for filename in glob.glob(os.path.join(directory, '*')):  \n",
      "        if os.path.isfile(filename):  \n",
      "            filepath = filename\n",
      "            mime_type = magic.from_file(filepath)\n",
      "            if \"application/pdf\" in mime_type:\n",
      "                print(f\"{filename}: PDF Document (Potential {mime_type})\")\n",
      "            else:\n",
      "                print(f\"{filename}: {mime_type}\")\n",
      "\n",
      "process_pdfs(directory)\n",
      "```\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Doc Classifier\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mDoc Classifier\u001b[0m (to chat_manager):\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "result = user_proxy.initiate_chat(\n",
    "    manager,\n",
    "    # message=\"Parse the documents and classify each of them by a heading\",\n",
    "    message=\"I have some pdf documents in the directory '../data/', and I would like to know what type of documents they are (in terms of classification)\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
